{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "from math import log, exp\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prob_prior: {'e': 0.3333333333333333, 'j': 0.3333333333333333, 's': 0.3333333333333333}\n",
      "prob_prior_smoothed: {'e': 0.3333333333333333, 'j': 0.3333333333333333, 's': 0.3333333333333333}\n",
      "prob_prior_smoothed_log: {'e': -1.0986122886681098, 'j': -1.0986122886681098, 's': -1.0986122886681098}\n"
     ]
    }
   ],
   "source": [
    "### 3.1 Obtaining priors for language classes\n",
    "\n",
    "path_data_lang = \"./data/languageID\"\n",
    "lang_classes = ['e', 'j', 's']\n",
    "lang_num = len(lang_classes)\n",
    "lang_str = ['english','japanese', 'spanish']\n",
    "lang_classes_to_str = { lang_classes[i]: lang_str[i] for i in range(len(lang_classes))} \n",
    "lang_classes_obs = { c:0 for c in lang_classes }\n",
    "obs_total = 0\n",
    "\n",
    "for entry in sorted(os.listdir(path_data_lang)):\n",
    "    entry_name = entry.split(sep=\".\")[0]\n",
    "    entry_lang_classes = entry_name[0]\n",
    "    entry_num = int(entry_name[1:])\n",
    "    if entry_num >= 10:\n",
    "        continue\n",
    "    \n",
    "    lang_classes_obs[entry_lang_classes] += 1\n",
    "    obs_total += 1\n",
    "    # print(f'entry: {entry}, entry_name: {entry_name}, entry_lang_classes: {entry_lang_classes}, entry_num: {entry_num}')\n",
    "\n",
    "\n",
    "prob_prior = { c: o/obs_total for c, o in lang_classes_obs.items() }\n",
    "smoothing_param = 1/2\n",
    "prob_prior_smoothed = { c: (o + smoothing_param)/(obs_total + lang_num*smoothing_param) for c, o in lang_classes_obs.items() }\n",
    "prob_prior_smoothed_log = { k: log(v) for k, v in prob_prior_smoothed.items() }\n",
    "print(f'prob_prior: {prob_prior}')\n",
    "print(f'prob_prior_smoothed: {prob_prior_smoothed}')\n",
    "print(f'prob_prior_smoothed_log: {prob_prior_smoothed_log}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.2 Obtaining estimates for class conditional probability\n",
    "char_classes = [ c for c in string.ascii_lowercase ]\n",
    "char_classes.append(\" \")\n",
    "char_classes_set = { c for c in char_classes }\n",
    "char_class_num = len(char_classes)\n",
    "\n",
    "char_counts_by_lang = {}\n",
    "for l in lang_classes:\n",
    "    char_counts = { c:0 for c in char_classes }\n",
    "    char_counts[\" \"] = 0\n",
    "    char_counts_by_lang[l] = char_counts\n",
    "\n",
    "# for l in char_counts_by_lang:\n",
    "#     print(f'{l}: {char_counts_by_lang[l]}')\n",
    "# print(char_counts_by_lang)\n",
    "num_obs = 0\n",
    "\n",
    "# Determine these counts for each training file\n",
    "for entry in sorted(os.listdir(path_data_lang)):\n",
    "    entry_name = entry.split(sep=\".\")[0]\n",
    "    entry_lang_classes = entry_name[0]\n",
    "    entry_num = int(entry_name[1:])\n",
    "    if entry_num >= 10:\n",
    "        continue\n",
    "\n",
    "    # if not entry_lang_classes == 'e':\n",
    "    #     continue\n",
    "\n",
    "    # print(f'Collecting character class observations from {entry_name}')\n",
    "\n",
    "    path_sample = path_to_data_lang + \"/\" + entry\n",
    "    with open(path_sample) as f:\n",
    "        file_text = f.read()\n",
    "        # print(file_text)\n",
    "        for c in file_text:\n",
    "            if not c in char_classes_set:\n",
    "                continue\n",
    "            char_counts_by_lang[entry_lang_classes][c] += 1\n",
    "            num_obs += 1\n",
    "    # print(char_counts)\n",
    "    # for l in char_counts_by_lang:\n",
    "    #     print(f'{l}: {char_counts_by_lang[l]}')\n",
    "    # print(num_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2872749586351235e-05"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 + 0.5) / (num_obs + 0.5*char_class_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_counts_by_lang:\n",
      "e: {'a': 910, 'b': 168, 'c': 325, 'd': 332, 'e': 1594, 'f': 286, 'g': 264, 'h': 714, 'i': 838, 'j': 21, 'k': 56, 'l': 438, 'm': 310, 'n': 876, 'o': 975, 'p': 253, 'q': 8, 'r': 814, 's': 1001, 't': 1212, 'u': 403, 'v': 140, 'w': 234, 'x': 17, 'y': 209, 'z': 9, ' ': 2712}\n",
      "j: {'a': 1885, 'b': 155, 'c': 78, 'd': 246, 'e': 861, 'f': 55, 'g': 200, 'h': 454, 'i': 1388, 'j': 33, 'k': 821, 'l': 20, 'm': 569, 'n': 811, 'o': 1304, 'p': 12, 'q': 1, 'r': 612, 's': 603, 't': 815, 'u': 1010, 'v': 3, 'w': 282, 'x': 0, 'y': 202, 'z': 110, ' ': 1766}\n",
      "s: {'a': 1695, 'b': 133, 'c': 608, 'd': 644, 'e': 1845, 'f': 139, 'g': 116, 'h': 73, 'i': 808, 'j': 107, 'k': 4, 'l': 858, 'm': 418, 'n': 878, 'o': 1175, 'p': 393, 'q': 124, 'r': 961, 's': 1066, 't': 577, 'u': 546, 'v': 95, 'w': 1, 'x': 40, 'y': 127, 'z': 43, ' ': 2728}\n",
      "prob_char_est:\n",
      "e: {'a': 0.0199537589989152, 'b': 0.0036927055368667886, 'c': 0.007133386660238218, 'd': 0.007286792824974524, 'e': 0.03494373281029136, 'f': 0.006278695170993086, 'g': 0.005796561510393268, 'h': 0.01565838638629864, 'i': 0.01837586701877034, 'j': 0.0004711760774043677, 'k': 0.0012382069010858966, 'l': 0.009609800462410011, 'm': 0.006804659164374706, 'n': 0.01920864334162457, 'o': 0.021378244814323752, 'p': 0.005555494680093359, 'q': 0.00018627891432265698, 'r': 0.01784990302538872, 's': 0.021948039140487173, 't': 0.026572139248967248, 'u': 0.008842769638728482, 'v': 0.0030790808779215654, 'w': 0.005139106518666243, 'x': 0.0003835154118407644, 'y': 0.004591227358893723, 'z': 0.0002081940807135578, ' ': 0.05944488883531848}\n",
      "j: {'a': 0.0413210462300435, 'b': 0.003407808373785078, 'c': 0.0017203405616857145, 'd': 0.005402088515357053, 'e': 0.01887991584576106, 'f': 0.0012162917346949956, 'g': 0.004393990861375615, 'h': 0.009960443124664424, 'i': 0.03042920853376579, 'j': 0.0007341580740951775, 'k': 0.018003309190125025, 'l': 0.00044926091101346686, 'm': 0.012480687259618019, 'n': 0.017784157526216017, 'o': 0.028588334556930124, 'p': 0.0002739395798862603, 'q': 3.2872749586351235e-05, 'r': 0.013423039414426755, 's': 0.013225802916908646, 't': 0.01787181819177962, 'u': 0.022145275638005283, 'v': 7.670308236815288e-05, 'w': 0.006191034505429483, 'x': 1.0957583195450412e-05, 'y': 0.0044378211941574165, 'z': 0.002421625886194541, ' ': 0.0387131414295263}\n",
      "s: {'a': 0.03715716461577234, 'b': 0.00292567471318526, 'c': 0.01333537874886315, 'd': 0.01412432473893558, 'e': 0.04044443957440747, 'f': 0.003057165711530665, 'g': 0.002553116884539946, 'h': 0.0016107647297312106, 'i': 0.017718412027043315, 'j': 0.0023558803870218385, 'k': 9.86182487590537e-05, 'l': 0.018814170346588357, 'm': 0.009171497134591994, 'n': 0.019252473674406374, 'o': 0.025761278092503916, 'p': 0.008623617974819473, 'q': 0.0027284382156671522, 'r': 0.021071432484851143, 's': 0.02337252495589573, 't': 0.012656008590745225, 'u': 0.0119766384326273, 'v': 0.0020928983903310285, 'w': 3.2872749586351235e-05, 'x': 0.0008875642388314833, 'y': 0.002794183714839855, 'z': 0.0009533097380041858, ' ': 0.0597955314975729}\n",
      "prob_char_est_log:\n",
      "e: {'a': -3.9143377323970157, 'b': -5.601395881772262, 'c': -4.942969169916054, 'd': -4.92169177146877, 'e': -3.3540161451145662, 'f': -5.070593095409884, 'g': -5.150490380266325, 'h': -4.156748634194757, 'i': -3.996717050301415, 'j': -7.660278696431061, 'k': -6.694090993412283, 'l': -4.644971819752441, 'm': -4.990147730191145, 'n': -3.952394927203389, 'o': -3.8453814715015127, 'p': -5.1929678085334405, 'q': -8.588265468068407, 'r': -4.025757203522579, 's': -3.8190774764588054, 't': -3.62789200875304, 'u': -4.728155143854675, 'v': -5.783124142790878, 'w': -5.270876043678345, 'x': -7.86613075063521, 'y': -5.383607892202486, 'z': -8.477039832958184, ' ': -2.8227056337159633}\n",
      "j: {'a': -3.1863833148662666, 'b': -5.681685899945389, 'c': -6.365233006776315, 'd': -5.220969638081933, 'e': -3.969656575596694, 'f': -6.71194861081229, 'g': -5.427517384818055, 'h': -4.609133717947145, 'i': -3.492352324817824, 'j': -7.216786192733657, 'k': -4.017199694087355, 'l': -7.707906745420316, 'm': -4.383572848677441, 'n': -4.029447244610033, 'o': -3.5547565269878953, 'p': -8.202602987256423, 'q': -10.322866523456515, 'r': -4.310782689145796, 's': -4.325585591027092, 't': -4.0245302095036095, 'u': -3.8101310947210028, 'v': -9.47556866306931, 'w': -5.084653080978183, 'x': -11.421478812124624, 'y': -5.417591745018084, 'z': -6.023316110606871, ' ': -3.2515761647654786}\n",
      "s: {'a': -3.292598669998985, 'b': -5.834230153724374, 'c': -4.317334719137096, 'd': -4.259856809185436, 'e': -3.207826109094625, 'f': -5.790267030303258, 'g': -5.970440358558923, 'h': -6.431046225345887, 'i': -4.033150952547517, 'j': -6.050840783996961, 'k': -9.224254234788404, 'l': -3.973144951227148, 'm': -4.691654741635149, 'n': -3.9501157239375266, 'o': -3.6588827635845544, 'p': -4.753250563707221, 'q': -5.904025915659917, 'r': -3.859837066535844, 's': -3.756194093653273, 't': -4.36962318916873, 'u': -4.424797323948085, 'v': -6.169205384077994, 'w': -10.322866523456515, 'x': -7.027029657452185, 'y': -5.880215266966197, 'z': -6.95557069347004, ' ': -2.8168243449383916}\n"
     ]
    }
   ],
   "source": [
    "### 3.2 Continued\n",
    "\n",
    "# We have collected per-character estimated probs for each lang class\n",
    "prob_char_est = { l: {c:((b + smoothing_param)/(num_obs + smoothing_param*char_class_num)) for c,b in char_counts_by_lang[l].items() } for l in lang_classes}\n",
    "\n",
    "prob_char_est_log = { l: {k:log(v) for k,v in prob_char_est[l].items()} for l in lang_classes}\n",
    "\n",
    "print(f'char_counts_by_lang:')\n",
    "for l in lang_classes:\n",
    "    print(f'{l}: {char_counts_by_lang[l]}')\n",
    "\n",
    "print(f'prob_char_est:')\n",
    "for l in lang_classes:\n",
    "    print(f'{l}: {prob_char_est[l]}')\n",
    "\n",
    "print(f'prob_char_est_log:')\n",
    "for l in lang_classes:\n",
    "    print(f'{l}: {prob_char_est_log[l]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e</th>\n",
       "      <th>j</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.019954</td>\n",
       "      <td>0.041321</td>\n",
       "      <td>0.037157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.003408</td>\n",
       "      <td>0.002926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>0.007133</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.013335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>0.007287</td>\n",
       "      <td>0.005402</td>\n",
       "      <td>0.014124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>0.034944</td>\n",
       "      <td>0.018880</td>\n",
       "      <td>0.040444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>0.006279</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.003057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>0.005797</td>\n",
       "      <td>0.004394</td>\n",
       "      <td>0.002553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>0.015658</td>\n",
       "      <td>0.009960</td>\n",
       "      <td>0.001611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>0.018376</td>\n",
       "      <td>0.030429</td>\n",
       "      <td>0.017718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j</th>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.002356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.018003</td>\n",
       "      <td>0.000099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l</th>\n",
       "      <td>0.009610</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.018814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>0.006805</td>\n",
       "      <td>0.012481</td>\n",
       "      <td>0.009171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>0.019209</td>\n",
       "      <td>0.017784</td>\n",
       "      <td>0.019252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o</th>\n",
       "      <td>0.021378</td>\n",
       "      <td>0.028588</td>\n",
       "      <td>0.025761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>0.005555</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.008624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q</th>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.002728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>0.017850</td>\n",
       "      <td>0.013423</td>\n",
       "      <td>0.021071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>0.021948</td>\n",
       "      <td>0.013226</td>\n",
       "      <td>0.023373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>0.026572</td>\n",
       "      <td>0.017872</td>\n",
       "      <td>0.012656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u</th>\n",
       "      <td>0.008843</td>\n",
       "      <td>0.022145</td>\n",
       "      <td>0.011977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v</th>\n",
       "      <td>0.003079</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.002093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w</th>\n",
       "      <td>0.005139</td>\n",
       "      <td>0.006191</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.000888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.004591</td>\n",
       "      <td>0.004438</td>\n",
       "      <td>0.002794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.002422</td>\n",
       "      <td>0.000953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>0.059445</td>\n",
       "      <td>0.038713</td>\n",
       "      <td>0.059796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          e         j         s\n",
       "a  0.019954  0.041321  0.037157\n",
       "b  0.003693  0.003408  0.002926\n",
       "c  0.007133  0.001720  0.013335\n",
       "d  0.007287  0.005402  0.014124\n",
       "e  0.034944  0.018880  0.040444\n",
       "f  0.006279  0.001216  0.003057\n",
       "g  0.005797  0.004394  0.002553\n",
       "h  0.015658  0.009960  0.001611\n",
       "i  0.018376  0.030429  0.017718\n",
       "j  0.000471  0.000734  0.002356\n",
       "k  0.001238  0.018003  0.000099\n",
       "l  0.009610  0.000449  0.018814\n",
       "m  0.006805  0.012481  0.009171\n",
       "n  0.019209  0.017784  0.019252\n",
       "o  0.021378  0.028588  0.025761\n",
       "p  0.005555  0.000274  0.008624\n",
       "q  0.000186  0.000033  0.002728\n",
       "r  0.017850  0.013423  0.021071\n",
       "s  0.021948  0.013226  0.023373\n",
       "t  0.026572  0.017872  0.012656\n",
       "u  0.008843  0.022145  0.011977\n",
       "v  0.003079  0.000077  0.002093\n",
       "w  0.005139  0.006191  0.000033\n",
       "x  0.000384  0.000011  0.000888\n",
       "y  0.004591  0.004438  0.002794\n",
       "z  0.000208  0.002422  0.000953\n",
       "   0.059445  0.038713  0.059796"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_3_3 = pd.DataFrame(prob_char_est)\n",
    "df_3_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 164, 'b': 32, 'c': 53, 'd': 57, 'e': 311, 'f': 55, 'g': 51, 'h': 140, 'i': 140, 'j': 3, 'k': 6, 'l': 85, 'm': 64, 'n': 139, 'o': 182, 'p': 53, 'q': 3, 'r': 141, 's': 186, 't': 225, 'u': 65, 'v': 31, 'w': 47, 'x': 4, 'y': 38, 'z': 2, ' ': 498}\n",
      "<164, 32, 53, 57, 311, 55, 51, 140, 140, 3, 6, 85, 64, 139, 182, 53, 3, 141, 186, 225, 65, 31, 47, 4, 38, 2, 498>\n"
     ]
    }
   ],
   "source": [
    "### 3.4 Representing e10.txt as a bag-of-words count vector x\n",
    "bag_of_words = { c:0 for c in char_classes }\n",
    "path_sample = path_to_data_lang + \"/\" + \"e10.txt\"\n",
    "with open(path_sample) as f:\n",
    "    file_text = f.read()\n",
    "    # print(file_text)\n",
    "    for c in file_text:\n",
    "        if not c in char_classes_set:\n",
    "            continue\n",
    "        bag_of_words[c] += 1\n",
    "        # num_obs += 1\n",
    "print(bag_of_words)\n",
    "\n",
    "print('<', end='')\n",
    "print(bag_of_words[char_classes[0]], end='')\n",
    "for i in range(1,len(char_classes)):\n",
    "    c = char_classes[i]\n",
    "    print(f', {bag_of_words[c]}', end='')\n",
    "print('>')\n",
    "\n",
    "# pd.DataFrame(bag_of_words.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-10904.720647602408, -11989.469322201065, -11338.321401996694]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 3.5 Compute \\hat p(x | y) for y=e,j,s under the multinomial assumption. Basically, compute the probability of the vector having arisen from each of the languages\n",
    "\n",
    "x = bag_of_words\n",
    "# We log the provided formula to avoid underflow\n",
    "prob_document = [ sum(x[c]*prob_char_est_log[l][c] for c in char_classes) for l in lang_classes]\n",
    "prob_document # <-- these values are logprobabilities, not normal probabilities\n",
    "# print([exp(v) for v in prob_document]) # <-- this would make them normal probabilities again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char_counts_by_lang:\n",
      "e: {'a': 910, 'b': 168, 'c': 325, 'd': 332, 'e': 1594, 'f': 286, 'g': 264, 'h': 714, 'i': 838, 'j': 21, 'k': 56, 'l': 438, 'm': 310, 'n': 876, 'o': 975, 'p': 253, 'q': 8, 'r': 814, 's': 1001, 't': 1212, 'u': 403, 'v': 140, 'w': 234, 'x': 17, 'y': 209, 'z': 9, ' ': 2712}\n",
      "j: {'a': 1885, 'b': 155, 'c': 78, 'd': 246, 'e': 861, 'f': 55, 'g': 200, 'h': 454, 'i': 1388, 'j': 33, 'k': 821, 'l': 20, 'm': 569, 'n': 811, 'o': 1304, 'p': 12, 'q': 1, 'r': 612, 's': 603, 't': 815, 'u': 1010, 'v': 3, 'w': 282, 'x': 0, 'y': 202, 'z': 110, ' ': 1766}\n",
      "s: {'a': 1695, 'b': 133, 'c': 608, 'd': 644, 'e': 1845, 'f': 139, 'g': 116, 'h': 73, 'i': 808, 'j': 107, 'k': 4, 'l': 858, 'm': 418, 'n': 878, 'o': 1175, 'p': 393, 'q': 124, 'r': 961, 's': 1066, 't': 577, 'u': 546, 'v': 95, 'w': 1, 'x': 40, 'y': 127, 'z': 43, ' ': 2728}\n",
      "{'a': 4490, 'b': 456, 'c': 1011, 'd': 1222, 'e': 4300, 'f': 480, 'g': 580, 'h': 1241, 'i': 3034, 'j': 161, 'k': 881, 'l': 1316, 'm': 1297, 'n': 2565, 'o': 3454, 'p': 658, 'q': 133, 'r': 2387, 's': 2670, 't': 2604, 'u': 1959, 'v': 238, 'w': 517, 'x': 57, 'y': 538, 'z': 162, ' ': 7206}\n",
      "{'a': 0.09841005467834014, 'b': 0.010004273457446226, 'c': 0.02216719080439618, 'd': 0.026791290912876256, 'e': 0.09424617306406899, 'f': 0.010530237450827845, 'g': 0.012721754089917927, 'h': 0.02720767907430337, 'i': 0.06650157241318855, 'j': 0.0035392993721304828, 'k': 0.019318219173579076, 'l': 0.028851316553620932, 'm': 0.028434928392193817, 'n': 0.05622335937585606, 'o': 0.0757059422973669, 'p': 0.014431137068408192, 'q': 0.00292567471318526, 'r': 0.052322459758275715, 's': 0.058524451846900645, 't': 0.05707805086510119, 'u': 0.04294276854297016, 'v': 0.005226767184229846, 'w': 0.011341098607291177, 'x': 0.0012601220674767974, 'y': 0.011801317101500093, 'z': 0.0035612145385213838, ' ': 0.1579316465960268}\n",
      "{'a': -2.318612298453674, 'b': -4.604742931529655, 'c': -3.8091419749568782, 'd': -3.619678410215651, 'e': -2.3618450575731442, 'f': -4.5535044031543315, 'g': -4.364441830426732, 'h': -3.604256026616457, 'i': -2.7105296862887687, 'j': -5.643826488901968, 'k': -3.9467066297267537, 'l': -3.5455996526283142, 'm': -3.5601370165246347, 'n': -2.878422961182659, 'o': -2.5808986236358287, 'p': -4.2383671103813425, 'q': -5.834230153724374, 'r': -2.950329559209793, 's': -2.8383106317848537, 't': -2.8633356343794323, 'u': -3.147887013924992, 'v': -5.2539623212362825, 'w': -4.479322106425154, 'x': -6.676546683761374, 'y': -4.439544134968235, 'z': -5.637653629794886, ' ': -1.845592956041521}\n"
     ]
    }
   ],
   "source": [
    "### 3.6 obtain estimates \\hat p(y | x) by using Bayes' rule\n",
    "# We must obtain the probabilities p(c) for individual characters so that we can calculate p(x)\n",
    "char_counts_all_lang = { c: sum(char_counts_by_lang[l][c] for l in lang_classes) for c in char_classes}\n",
    "\n",
    "print(f'char_counts_by_lang:')\n",
    "for l in lang_classes:\n",
    "    print(f'{l}: {char_counts_by_lang[l]}')\n",
    "\n",
    "print(char_counts_all_lang)\n",
    "num_char_all = sum(v for c,v in char_counts_all_lang.items())\n",
    "# num_char_all\n",
    "prob_char_prior = { c: (v + smoothing_param)/(num_char_all + smoothing_param * char_class_num) for c,v in char_counts_all_lang.items() }\n",
    "print(prob_char_prior)\n",
    "\n",
    "prob_char_prior_log = { c: log(v) for c,v in prob_char_prior.items() }\n",
    "print(prob_char_prior_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### 3.6 continued\n",
    "\n",
    "path_sample = path_to_data_lang + \"/\" + \"e10.txt\"\n",
    "file_text = ''\n",
    "with open(path_sample) as f:\n",
    "    file_text = f.read()\n",
    "\n",
    "def pred_label(file_text):\n",
    "\n",
    "    bag_of_words = { c:0 for c in char_classes }\n",
    "\n",
    "    # for a string file_text, obtain probability of observing sequence of chars and obtain the bag of words\n",
    "    p_x_log = 0\n",
    "    for c in file_text:\n",
    "        if not c in char_classes_set:\n",
    "            continue\n",
    "        p_x_log += prob_char_prior_log[c]\n",
    "        bag_of_words[c] += 1\n",
    "\n",
    "    # then, for each language, obtain the probability that a document is of a language class given the string\n",
    "    p_y_probs = []\n",
    "    for l in lang_classes:\n",
    "        p_x_given_y_log = sum(bag_of_words[c]*prob_char_est_log[l][c] for c in char_classes)\n",
    "        p_y_log = prob_prior_smoothed_log[l]\n",
    "        p_y_given_x_log = p_x_given_y_log + p_y_log - p_x_log\n",
    "        # print(f'{p_y_given_x_log} = {p_x_given_y_log} + {p_y_log} - {p_x_log}')\n",
    "        p_y_probs.append(p_y_given_x_log)\n",
    "        # print(p_y_given_x_log)\n",
    "    \n",
    "    # for p in p_y_probs:\n",
    "    #     print(p)\n",
    "    \n",
    "    highest_prob = max(enumerate(p_y_probs), key = lambda x: x[1])\n",
    "    return lang_classes[highest_prob[0]]\n",
    "\n",
    "pred_label(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.7 Evaluate the performance of our classifier on the test set (files 10-19.txt in the three languages) and present the performance using a confusion matrix\n",
    "\n",
    "confusion_matrix = { l_true:{ l_pred:0 for l_pred in lang_classes} for l_true in lang_classes }\n",
    "\n",
    "for entry in sorted(os.listdir(path_data_lang)):\n",
    "    entry_name = entry.split(sep=\".\")[0]\n",
    "    entry_lang_classes = entry_name[0]\n",
    "    entry_num = int(entry_name[1:])\n",
    "    if entry_num < 10:\n",
    "        continue\n",
    "\n",
    "    path_sample = path_to_data_lang + \"/\" + entry\n",
    "    file_text = ''\n",
    "    with open(path_sample) as f:\n",
    "        file_text = f.read()\n",
    "    \n",
    "    prediction = pred_label(file_text)\n",
    "    # print(f'entry_lang_classes: {entry_lang_classes}, prediction={prediction}')\n",
    "    confusion_matrix[entry_lang_classes][prediction] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>e</th>\n",
       "      <th>j</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    e   j   s\n",
       "e  10   0   0\n",
       "j   0  10   0\n",
       "s   0   0  10"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_confusion_matrix = pd.DataFrame(confusion_matrix)\n",
    "df_confusion_matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
