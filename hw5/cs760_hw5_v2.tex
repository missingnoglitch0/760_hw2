\documentclass[a4paper]{article}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{paralist}
\usepackage{epstopdf}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyvrb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{float}
\usepackage{paralist}
\usepackage[svgname]{xcolor}
\usepackage{enumerate}
\usepackage{array}
\usepackage{times}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{comment}
\usepackage{environ}
\usepackage{times}
\usepackage{textcomp}
\usepackage{caption}
\usepackage{multirow}
\usepackage{bbm}

% \usepackage{kky}

\newcounter{thm}
\ifx\fact\undefined
\newtheorem{fact}[thm]{Fact}
\fi

\newcommand{\pen}{{\rm pen}}
\newcommand{\diag}{{\rm diag}}
\newcommand{\diam}{{\bf{\rm diam}}}
\newcommand{\spann}{{\bf{\rm span}}}
\newcommand{\nulll}{{\bf{\rm null}}}
% Distributions
\newcommand{\Bern}{{\bf{\rm Bern}}\,} % support of a function
\newcommand{\Categ}{{\bf{\rm Categ}}\,} % support of a function
\newcommand{\Mult}{{\bf{\rm Mult}}\,} % support of a function
\newcommand{\Dir}{{\bf{\rm Dir}}\,} % support of a function
\newcommand{\horizontalline}{\noindent\rule[0.5ex]{\linewidth}{1pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} 
\newcommand{\Hrule}{\rule{\linewidth}{0.3mm}}
\newcommand{\HRuleN}{\HRule\\} 
\newcommand{\HruleN}{\Hrule\\}
\newcommand{\superscript}[1]{{\scriptsize \ensuremath{^{\textrm{#1}}}}}
\newcommand{\supindex}[2]{#1^{(#2)}}
\newcommand{\xii}[1]{\supindex{x}{#1}}
\newcommand{\yii}[1]{\supindex{y}{#1}}
\newcommand{\zii}[1]{\supindex{z}{#1}}
\newcommand{\Xii}[1]{\supindex{X}{#1}}
\newcommand{\Yii}[1]{\supindex{Y}{#1}}
\newcommand{\Zii}[1]{\supindex{Z}{#1}}
\newcommand{\NN}{\mathbb{N}} % Natural numbers
\newcommand{\Ncal}{\mathcal{N}}
\newcommand{\Dcal}{\mathcal{D}}
\newcommand{\Lcal}{\mathcal{L}}
\newcommand{\Xcal}{\mathcal{X}}
\newcommand{\Pcal}{\mathcal{P}}
\newcommand{\Jcal}{\mathcal{J}}
\newcommand{\Rcal}{\mathcal{R}}
\newcommand{\indfone}{\mathbbm{1}}
\newcommand{\gb}{\mathbf{g}}
\newcommand{\Hb}{\mathbf{H}}
\newcommand{\Db}{\mathbf{D}}
\newcommand*{\zero}{{\bf 0}}
\newcommand*{\one}{{\bf 1}}

% Stuff mostly appearing in Statistics
\newcommand{\Xbar}{\bar{X}}
\newcommand{\Ybar}{\bar{Y}}
\newcommand{\Zbar}{\bar{Z}}
\newcommand{\Xb}{\mathbf{X}}


%%%%  brackets
\newcommand{\inner}[2]{\left\langle #1,#2 \right\rangle}
\newcommand{\rbr}[1]{\left(#1\right)}
\newcommand{\sbr}[1]{\left[#1\right]}
\newcommand{\cbr}[1]{\left\{#1\right\}}
\newcommand{\nbr}[1]{\left\|#1\right\|}
\newcommand{\abr}[1]{\left|#1\right|}

% derivatives and partial fractions
\newcommand{\differentiate}[2]{ \frac{ \ud #2}{\ud #1} }
\newcommand{\differentiateat}[3]{ \frac{ \ud #2}{\ud #1}  \Big|_{#1=#3} }
\newcommand{\partialfrac}[2]{ \frac{ \partial #2}{\partial #1} }
\newcommand{\partialfracat}[3]{ \frac{ \partial #2}{\partial #1} \Big|_{#1=#3} }
\newcommand{\partialfracorder}[3]{ \frac{ \partial^{#3} #2}{\partial^{#3} #1} }
\newcommand{\partialfracatorder}[4]{ \frac{ \partial^{#3} #2}{\partial^{#3} #1} \Big|_{#1=#4} }

\urlstyle{rm}

\setlength\parindent{0pt} % Removes all indentation from paragraphs
\theoremstyle{definition}
\newtheorem{definition}{Definition}[]
\newtheorem{conjecture}{Conjecture}[]
\newtheorem{example}{Example}[]
\newtheorem{theorem}{Theorem}[]
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}


\floatname{algorithm}{Procedure}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\Nat}{\mathbb{N}}
\newcommand{\br}[1]{\{#1\}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\renewcommand{\qedsymbol}{$\blacksquare$}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}

\newcommand{\vc}[1]{\boldsymbol{#1}}
\newcommand{\xv}{\vc{x}}
\newcommand{\Sigmav}{\vc{\Sigma}}
\newcommand{\alphav}{\vc{\alpha}}
\newcommand{\muv}{\vc{\mu}}

\newcommand{\red}[1]{\textcolor{red}{#1}}

\def\x{\mathbf x}
\def\y{\mathbf y}
\def\w{\mathbf w}
\def\v{\mathbf v}
\def\E{\mathbb E}
\def\V{\mathbb V}

\hypersetup{
%    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\geometry{
  top=1in,            % <-- you want to adjust this
  inner=1in,
  outer=1in,
  bottom=1in,
  headheight=3em,       % <-- and this
  headsep=2em,          % <-- and this
  footskip=3em,
}


\pagestyle{fancyplain}
\lhead{\fancyplain{}{Homework 5}}
\rhead{\fancyplain{}{CS 760 Machine Learning}}
\cfoot{\thepage}

\title{\textsc{Homework 5}} % Title

%%% NOTE:  Replace 'NAME HERE' etc., and delete any "\red{}" wrappers (so it won't show up as red)

\author{
\red{$>>$Martin Diges$<<$} \\
\red{$>>$9080689699$<<$}\\
}

\date{}

\begin{document}

\maketitle 


\textbf{Instructions:}
Use this latex file as a template to develop your homework. Submit your homework on time as a single pdf file. Please wrap your code and upload to a public GitHub repo, then attach the link below the instructions so that we can access it. Answers to the questions that are not within the pdf are not accepted. This includes external links or answers attached to the code implementation. Late submissions may not be accepted. You can choose any programming language (i.e. python, R, or MATLAB). Please check Piazza for updates about the homework. It is ok to share the experiments results and compare them with each other.

\\\\
\hypersetup{colorlinks=true, linkcolor=cyan}
\url{https://github.com/missingnoglitch0/cs760/tree/main/hw5}

\vspace{0.1in}


\section{Clustering}

\subsection{K-means Clustering (14 points)}

\begin{enumerate}

\item \textbf{(6 Points)}
Given $n$ observations $X_1^n = \{X_1, \dots, X_n\}$, $X_i \in \Xcal$, the K-means objective
is to find $k$
($<n$) centres $\mu_1^k = \{\mu_1, \dots, \mu_k\}$, and a rule $f:\Xcal \rightarrow
\{1,\dots, K\}$ so as to minimize the objective

\begin{equation}
J(\mu_1^K, f; X_1^n) = \sum_{i=1}^n \sum_{k=1}^K \indfone(f(X_i) = k) \|X_i - \mu_k\|^2
\label{eqn:kmeans}
\end{equation}

Let $\Jcal_K(X_1^n) = \min_{\mu_1^K, f} J(\mu_1^K, f; X_1^n)$. Prove that
$\Jcal_{K}(X_1^n)$ is a non-increasing function of $K$.

\textbf{ANSWER}

Suppose $\Jcal_K(X_1^n) = e'$ for some $\mu_1^K'$ and some $f'$. \\
For $K+1$, we can choose
\begin{itemize}
    \item $f''(X_i) = f'(X_i)$
    \item $\mu_1^{K+1}'' = \mu_1^K' + \mu_{K+1}$ where $\mu_{K+1}$ is such that $f''(X_i) = f'(X_i) \neq K+1$. For the traditional algorithm, this means $\mu_{K+1}$ satisfies $||X_i - \mu_{K+1}||^2 > ||X_i - \mu_{i}||^2$ for $i \leq K$
\end{itemize}
We see then that

$$ J(\mu_1^{K+1}'', f''; X_1^n) = \sum_{i=1}^n \sum_{k=1}^{K+1} \indfone(f''(X_i) = k) \|X_i - \mu_k\|^2 $$
$$ = \sum_{i=1}^n \left( \left( \sum_{k=1}^{K} \indfone(f''(X_i) = k) \|X_i - \mu_k\|^2 \right) + \indfone(f''(X_i) = K+1) \|X_i - \mu_{K+1}\|^2 \right)$$
$$ = \sum_{i=1}^n \left( \left( \sum_{k=1}^{K} \indfone(f''(X_i) = k) \|X_i - \mu_k\|^2 \right) + 0 \right) $$
$$ = \sum_{i=1}^n \sum_{k=1}^{K} \indfone(f''(X_i) = k) \|X_i - \mu_k\|^2 $$
$$ = \sum_{i=1}^n \sum_{k=1}^{K} \indfone(f'(X_i) = k) \|X_i - \mu_k\|^2 = J(\mu_1^{K}', f'; X_1^n)$$

$\Jcal_{K+1}(X_1^n) = \min_{\mu_1^{K+1}, f} J(\mu_1^{K+1}, f; X_1^n) \leq J(\mu_1^{K+1}'', f''; X_1^n) = J(\mu_1^{K}', f'; X_1^n) = \Jcal_K(X_1^n)$
$$ \Jcal_{K+1}(X_1^n) \leq \Jcal_K(X_1^n) $$

\item \textbf{(8 Points)}
Consider the K-means (Lloyd's) clustering algorithm we studied in class. We
terminate the algorithm when there are no changes to the objective.
Show that the algorithm terminates in a finite number of steps.

\textbf{ANSWER}

From class, we have the following K-means algorithm:
\begin{enumerate}
    \item Select k cluster centers
    \item For each point, determine its cluster assignment by finding the closest center in Euclidean space
    \item Update all cluster centers as the centroids of the points assigned to them
    \item Repeat steps 2, 3 until there are no changes to the objective
\end{enumerate}

We define $J_{beg}$ as the objective before an iteration of the algorithm (before step 2). \\
We define $J_{end}$ as the objective calculated at the end of step 3.

If there was no change in point assignment, then $J_{end} = J_{beg}$ $\leftrightarrow$\\
If $J_{end} \neq J_{beg}$, then there must have been some change in point assignments during the step.
\begin{itemize}
    \item If $J_{end} > J_{beg}$, then at least one point $X_i$'s assignment changed from $f(X_i) = a$ to $f(X_i) = b$ and resutled in $||X_i - \mu_b||^2 > ||X_i - \mu_a||^2$. However, we know $||X_i - \mu_b|| \leq ||X_i - \mu_a||$ for the assignment to have changed, which is a contradiction. Thus, it cannot be that $J_{end} > J_{beg}$, meaning $J_{end} \leq J_{beg}$.
    \item If $J_{end} = J_{beg}$, the algorithm terminates.
    \item If $J_{end} < J_{beg}$, the algorithm advances to the next iteration. However, the point assignments at the end of Step 3 $a_{cur}$ must be different from other assignments examined in the past $a_{past}$, i.e. $a_{cur} \neq a_{past}$. This is because $a_{cur} = a_{past} \implies (J_{end} < J_{beg} \leq J_{past} \implies J_{end} < J_{end})$, which is a contradiction. Thus, the number of times the algorithm advances to the next iteration must be bounded by the number of distinct point assignments, which is finite.
\end{itemize}

\end{enumerate}



\subsection{Experiment (20 Points)}

In this question, we will evaluate
K-means clustering and GMM on a simple 2 dimensional problem.
First, create a two-dimensional synthetic dataset of 300 points by sampling 100 points each from the
three Gaussian distributions shown below:
\[
P_a = \Ncal\left(
\begin{bmatrix}
-1 \\ -1
\end{bmatrix},
\;
\sigma\begin{bmatrix}
2, &0.5 \\ 0.5, &1
\end{bmatrix}
\right),
\quad
P_b = \Ncal\left(
\begin{bmatrix}
1 \\ -1
\end{bmatrix},
\;
 \sigma\begin{bmatrix}
1, &-0.5 \\ -0.5, &2
\end{bmatrix}
\right),
\quad
P_c = \Ncal\left(
\begin{bmatrix}
0 \\ 1
\end{bmatrix},
\;
 \sigma\begin{bmatrix}
1 &0 \\ 0, &2
\end{bmatrix}
\right)
\]
Here, $\sigma$ is a parameter we will change to produce different datasets.\\

First implement K-means clustering and the expectation maximization algorithm for GMMs.
Execute both methods on five synthetic datasets,
generated as shown above with $\sigma \in \{0.5, 1, 2, 4, 8\}$. Finally, evaluate both methods on \emph{(i)} the clustering objective~\eqref{eqn:kmeans} and \emph{(ii)}  the clustering accuracy. For each of the two criteria, plot the value achieved by each method against $\sigma$.\\


Guidelines:
\begin{itemize} 
\item Both algorithms are only guaranteed to find only a local optimum so we recommend trying multiple
restarts and picking the one with the lowest objective value (This is~\eqref{eqn:kmeans} for K-means and the negative log likelihood for GMMs).
You may also experiment with a smart initialization
strategy (such as kmeans++).

\item
To plot the clustering accuracy,  you may treat the `label' of points generated from distribution
$P_u$ as $u$, where $u\in \{a, b, c\}$.
Assume that the cluster id $i$ returned by a method is $i\in \{1, 2, 3\}$.
Since clustering is an unsupervised learning problem, you should obtain the best possible mapping
from $\{1, 2, 3\}$ to $\{a, b, c\}$ to compute the clustering objective.
One way to do this is to compare the clustering centers returned by the method (centroids for
K-means, means for GMMs) and map them to the distribution with the closest mean.

\end{itemize}

Points break down: 7 points each for implementation of each method, 6 points for reporting of
evaluation metrics.

\textbf{ANSWER}

Implementations for each method can be found in the GitHub repository, in clustering.ipynb
Running the entire notebook should yield results similar to below within a minute:

For the kmeans algorithm:

\includegraphics[width=2in]{hw5/1_2_kmeans.png} \hspace{0.4in}

For the GMM algorithm:

\includegraphics[width=2in]{hw5/1_2_gmm.png} \hspace{0.4in}



\section{Linear Dimensionality Reduction}

\subsection{Principal Components Analysis  (10 points)}
\label{sec:pca}

Principal Components Analysis (PCA) is a popular method for linear dimensionality reduction. PCA attempts to find a lower dimensional subspace such that when you project the data onto the subspace as much of the information is preserved. Say we have data $X = [x_1^\top; \dots; x_n^\top] \in \RR^{n\times D}$ where  $x_i \in \RR^D$. We wish to find a $d$ ($ < D$) dimensional subspace $A = [a_1, \dots, a_d] \in \RR^{D\times d}$, such that $ a_i \in \RR^D$ and $A^\top A = I_d$, so as to maximize $\frac{1}{n} \sum_{i=1}^n \|A^\top x_i\|^2$.
\begin{enumerate}

\item  \textbf{(4 Points)}
Suppose we wish to find the first direction $a_1$ (such that $a_1^\top a_1 = 1$) to maximize $\frac{1}{n} \sum_i (a_1^\top x_i)^2$.
Show that $a_1$ is the first right singular vector of $X$.

\textbf{ANSWER}

My linear algebra skills are not well developed enough to rigorously prove the statement above at this moment, but here is an explanation in case it is of any aid.

From doing SVD, we get matrices U, $\Sigma$, V. The first right vector of X is the first vector of V and is associated with the greatest singular value in $\Sigma$. Thanks to V being unitary, $a_1$ satisfies $a_1^\top a_1 = 1$. Furthermore, since we know we can reconstruct $X = U \Sigma V$, then the greatest $\Sigma V'$ where $V'$ only has one nonzero column will be achieved where that column is $a_1$, as this maximizes the product of a unitary vector and some scalar, in this case the greatest value in $\Sigma$.

% 1.2 Are my expressions sensible for GMM?

% My understanding of the singular vector based on online sources: the vector which maximises X*v
% I do not understand the intuition. I've seen that the singular vector is the vector which fits the data through the origin, but this is not obvious and the definition is kinda dookie


% X @ X^T <-- yields square matrix DxD, find eigenvectors. These should be of length D, which may be what we seek
% first <-- biggest eigenvalue

% SVD gives U, SIGMA, V, which are all unitary
% where U and V satisfy U^T U = I_d, so forth for V
% V is eigenvectors of X X^T
% U is eigenvectros of X^T X

% Sigma is a diagonal matrix, it is a matrix with the (square root of the) eigenvalues on the diagonal

% A = U Sigma V^T

% I'm guessing part 2 follows somewhat naturally from part 1



% 2.2 Seems to me like we're minimizing the sum of squares, hence doing multiple regression where we have vectors instead of parameters, as well as one b vector constant

% I'm guessing we do exactly as the hint says, applying SVD to x, taking its first d singular values. This should be our A. Lastly, we can express our x_i as the elementwise product of z_i and A
 

\item  \textbf{(6 Points)}
Given $a_1, \dots, a_k$, let $A_k = [a_1, \dots, a_k]$ and 
$\tilde{x}_i = x_i - A_kA_k^\top x_i$. We wish to find $a_{k+1}$, to maximize
$\frac{1}{n} \sum_i (a_{k+1}^\top \tilde{x}_i)^2$. Show that $a_{k+1}$ is the
$(k+1)^{th}$ right singular vector of $X$.


\end{enumerate}



%%% Question 2.2 

\subsection{Dimensionality reduction via optimization (22 points)}

We will now motivate the dimensionality reduction problem from a slightly different
perspective. The resulting algorithm has many similarities to PCA.
We will refer to method as DRO.

As before, you are given data $\{x_i\}_{i=1}^n$, where $x_i \in \RR^D$. Let $X=[x_1^\top; \dots
x_n^\top] \in \RR^{n\times D}$. We suspect that the data
actually lies approximately in  a $d$ dimensional affine subspace.
Here $d<D$ and $d<n$.
Our goal, as in PCA, is to use this dataset to find a $d$ dimensional representation $z$ for each $x\in\RR^D$.
(We will assume that the span of the data has dimension larger than
$d$, but our method should work whether $n>D$ or $n<D$.)


Let $z_i\in \RR^d$ be the lower dimensional representation for $x_i$ and
let $Z = [z_1^\top; \dots; z_n^\top] \in \RR^{n\times d}$.
We wish to find parameters $A \in \RR^{D\times d}$, $b\in\RR^D$ and the lower
dimensional representation $Z\in \RR^{n\times d}$ so as to minimize 
\begin{equation}
J(A,b,Z) = \frac{1}{n} \sum_{i=1}^n \|x_i - Az_i - b\|^2 = \| X - ZA^\top - \one b^\top\|_F^2.
\label{eqn:dimobj}
\end{equation}
Here, $\|A\|^2_F = \sum_{i,j} A_{ij}^2$ is the Frobenius norm of a matrix.


\begin{enumerate}

%%% 2.2.1

\item \textbf{(3 Points)}
Let $M\in\RR^{d\times d}$ be an arbitrary invertible matrix and $p\in\RR^{d}$ be an arbitrary vector.
Denote, $A_2 = A_1M^{-1}$, $b_2 = b_1- A_1M^{-1}p$ and $Z_2 = Z_1 M^\top +
\one p^\top$.
Show that both
$(A_1, b_1, Z_1)$ and $(A_2, b_2, Z_2)$ achieve the same objective value $J$~\eqref{eqn:dimobj}.
\end{enumerate}

Therefore, in order to make the problem determined, we need to impose some
constraint on $Z$. We will assume that the $z_i$'s have zero mean and identity covariance.
That is,
\begin{align*}
\Zbar = \frac{1}{n} \sum_{i=1}^n z_i =\frac{1}{n} Z^\top {\bf 1}_n = 0, \hspace{0.3in} 
S = \frac{1}{n} \sum_{i=1}^n z_i z_i^\top 
= \frac{1}{n} Z^\top Z
= I_d
\end{align*}
Here, ${\bf 1}_d = [1, 1 \dots, 1]^\top \in\RR^d$ and $I_d$  is the $d\times d$ identity matrix.

\begin{enumerate}
\setcounter{enumi}{1}


%%% 2.2.2

\item \textbf{(16 Points)}
Outline a procedure to solve the above problem. Specify how you
would obtain $A, Z, b$ which minimize the objective and satisfy the constraints.

\textbf{Hint: }The rank $k$ approximation of a matrix in Frobenius norm is obtained by
taking its SVD and then zeroing out all but the first $k$ singular values.

\textbf{ANSWER}

We are trying to minimize the objective 
\begin{equation}
J(A,b,Z) = \frac{1}{n} \sum_{i=1}^n \|x_i - Az_i - b\|^2 = \| X - ZA^\top - \one b^\top\|_F^2.
\label{eqn:dimobj}
\end{equation}
This can be thought of as the Frobenius norm of the difference between the data and our reconstruction/approximation of the data. As we want to minimize this, we know our objective is to obtain $Z, A, b$ s.t. $ ZA^\top + \one b^\top \approx X$.

Per the provided hint, we can obtain an approximation of $X$ by taking its SVD and zeroing out all but the first $k$ singular values.

One way to proceed which I tried was to set b = the mean of X, then projecting the demeaned X onto the first $k$ singular vectors (A) to obtain Z.

The below is speculation:
I noticed that this problem appeared close in its formulation to least squares regression. We want to regress X on our singular vectors A such that we obtain representations Z and intercept b. Like regression, but instead of obtaining parameters we obtain vectors. My idea, which I did not have time to implement, was to run this regression. 

%%% 2.2.3

\item \textbf{(3 Points)}
You are given a point $x_*$ in the original $D$ dimensional space.
State the rule to obtain the $d$ dimensional
representation $z_*$ for this new point.
(If $x_*$ is some original point $x_i$ from the $D$--dimensional space, it shoud be the
$d$--dimensional representation $z_i$.)

$z_i = (x_i - b) A^\top$
 
\end{enumerate}


\subsection{Experiment (34 points)}

\textbf{FOR ANSWERS, PLEASE GO TO THE BOTTOM OF THE DOCUMENT}

Here we will compare the above three methods on two data sets. 

\begin{itemize}
\item We will implement three variants of PCA:
\begin{enumerate}
    \item "buggy PCA": PCA applied directly on the matrix $X$.
    \item "demeaned PCA": We subtract the mean along each dimension before applying PCA.
    \item "normalized PCA": Before applying PCA, we subtract the mean and scale each dimension so that the sample  mean and standard deviation along each dimension is $0$ and $1$ respectively.
    
\end{enumerate}



\item 
One way to study how well the low dimensional representation $Z$ captures the linear
structure in our data is to project $Z$ back to $D$ dimensions and look at the reconstruction
error. For PCA, if we mapped it to $d$ dimensions via $z = Vx$ then the
reconstruction is $V^\top z$. For the preprocessed versions, we first do this and then
reverse the preprocessing steps as well. For DRO  we just compute $Az + b$.
We will compare all methods by the reconstruction error on the datasets.

\item 
Please implement code for the methods: Buggy PCA (just take the SVD of $X$)
, Demeaned PCA,
Normalized PCA, DRO. In all cases your function should take in
an $n \times d$ data matrix and $d$ as an argument. It should return the
the $d$ dimensional representations, the estimated parameters, and the
reconstructions of these representations in $D$ dimensions. 

\item
You are given two datasets: A two Dimensional dataset with $50$ points 
\texttt{data2D.csv} and a thousand dimensional dataset with $500$ points
\texttt{data1000D.csv}. 

\item
For the $2D$ dataset use $d=1$. For the $1000D$ dataset, you need to choose
$d$. For this, observe the singular values in DRO and see if there is a clear
``knee point" in the spectrum.
Attach any figures/ Statistics you computed to justify your choice.

\item
For the $2D$ dataset you need to attach the a 
plot comparing the orignal points with the reconstructed points for all 4
methods.
For both datasets you should also report the reconstruction errors, that is the squared sum of
differences $\sum_{i=1}^n \|x_i - r(z_i)\|^2$,
where $x_i$'s are the original points and $r(z_i)$ are the $D$ dimensional points
reconstructed from the 
$d$ dimensional representation $z_i$.

\item \textbf{Questions:} After you have completed the experiments, please answer the following questions.
\begin{enumerate}
\item Look at the results for Buggy PCA. The reconstruction error is bad and the
reconstructed points don't seem to well represent the original points. Why is
this? \\
\textbf{Hint: } Which subspace is Buggy PCA trying to project the points
onto?

\textbf{ANSWER}

With PCA, we try to find a direction vector which captures as much variance as possible. However, since we do not demean for Buggy PCA, there is no frame of reference for what the points are varying relative to. Thus, the points further away from the origin contribute more toward the principal component. This results in the direction vector being roughly in the direction of the cloud of points, as most of the variance relative to the origin is achieved in that direction.

\item The error criterion we are using is the average squared error 
between the original points and the reconstructed points.
In both examples DRO and demeaned PCA achieves the lowest error among all
methods. 
Is this surprising? Why?

\textbf{ANSWER}

It is not terribly surprising because the methods are somewhat analogous to each other. 
For one, PCA extracts the $d$ largest eigenvalues and accompanying eigenvectors.
DRO extracts the $d$ largest singular values and accompanying singular vectors for its calculations.
Both demeaned PCA and DRO keep a vector (mean of data for PCA, vector b for DRO) which is subtracted and added back during "training" and reconstruction respectively.

\end{enumerate}

\item Point allocation:
\begin{itemize}
\item Implementation of the three PCA methods: \textbf{(6 Points)}

\textbf{ANSWER}

Please see the GitHub repository, specifically the PCA.ipynb file. The PCA methods are implemented within a PCA class which supports "training" and reconstruction of points, including the necessary modifications for each variant (buggy, demeaned, normalized). Fields V and Z within the class allow parameters to easily be retrieved.

\item Implementation of DRO: \textbf{(6 points)}

\textbf{ANSWER}

Please see the PCA.ipynb file referenced above for the DRO class with analogous functionality to the PCA class.

\item Plots showing original points and reconstructed points for 2D dataset for each one of the 4 methods: \textbf{(10 points)}

\textbf{ANSWER}

\includegraphics[width=4in]{hw5/PCA_buggy.png} \\
\includegraphics[width=4in]{hw5/PCA_demeaned.png} \\
\includegraphics[width=4in]{hw5/PCA_normalized.png} \\
\includegraphics[width=4in]{hw5/DRO.png} \\

\newpage
\item Implementing reconstructions and reporting results for each one of the 4 methods for the 2 datasets: \textbf{(5 points)}

\textbf{ANSWER}
\\
Please see the reconstruct() functions within each of the classes for implementations of reconstructions.

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{recon_error_2D.png}
    \caption{for data_2D.csv}
    \label{fig:enter-label}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{recon_error_1000D.png}
    \caption{for data_1000D.csv}
    \label{fig:enter-label}
\end{figure}


\item Choice of $d$ for $1000D$ dataset and appropriate justification:
\textbf{(3 Points)}

\textbf{ANSWER}

\includegraphics[width=4in]{hw5/choice_of_d.png} \\

Based on the graph shown here (and of course checking the table to ensure the right dimension), I chose dimension of 30, since the gains in reduced error of adding additional dimensions sharply dropped off.

\item Questions \textbf{(4 Points)}

\textbf{Questions answered where they are stated (i.e. before the "Point allocation" section}

\end{itemize}

\end{itemize}




\bibliographystyle{apalike}
\end{document}


